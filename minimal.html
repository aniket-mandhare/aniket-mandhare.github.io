<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Project 3</title>
        <style>
           
            video {
                border: none; 
                width: 100%;
                height: 100%; 
                object-fit: contain; 
            }
    
          
    
body {
    font-family: Arial, sans-serif;
    font-weight: 300; 
    margin: 0;
    padding: 0;
    color: #58585b;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    min-height: 100vh;
    
   
    background-image: url("");
    background-size: contain; 
    background-position: cover; 
    background-repeat: repeat;
}
 
  h2 {
            color: #333; 
        }
        h1, h2, h3, h4, h5, h6, li {
    font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;
    letter-spacing: 0.02em;
}


h2, h3, li, p {
    line-height: 1.9; 
}
header {
    text-align: center;
    padding: 1em;
    background-color: #333;
    color: #fff;
    width: 100%;
}

.project {
    display: flex;
    flex-direction: column; 
    background-color: #fcfcfe;
    align-items: center; 
    text-align: center;
    border: 1px solid #ddd;
    border-radius: 10px;
    overflow: hidden;
    max-width: 1200px;
    width: 60%; 
    margin: 2em 20%; 
}
.video-container {
    display: flex; 
    flex-direction: row;
    width: 200%;
    height: auto;
    justify-content: space-between;

}

.video-container video {
    flex: 1;
    width: 50%;
    height: auto; 
    object-fit: contain;
}


.project video 
{
    
    
    width: 100%;
    height: auto;
    display: block; 
}

.project a {
    display: block;
    text-decoration: none;
}

.project img, .project video {
    width: 50%; 
    height: auto; 
    object-fit: cover;
    border-bottom: 1px solid #ddd;
}

.project .description {
    padding: 1em;
    text-align: left;
}

.project-images {
    display: flex;
    justify-content: center;
    gap: 10px; 
    width: 100%; 
    margin-top: 10px; 
}

.project-images img {
    width: 50%;
    height: auto;
}

footer {
    text-align: center;
    padding: 1em;
    background-color: #333;
    color: #ffffff;
    width: 100%;
    margin-top: auto;
}

.big-image-container {
    display: flex; 
    justify-content: center; 
    gap: 0; 
    width: 100%; 
    margin: 0 auto; 
    
}

.big-image {
    width: 40%;
    height: auto;
    object-fit: contain;
    margin: 1px 0;
    margin-bottom: 0;
}

.small-image {
    width: 30%;
    height: auto; 
}


   
    strong {
        color: #000000; 
    }

    a {
        color: #0066cc; 
        text-decoration: none; 
    }

  
    a:hover {
        text-decoration: underline; 
    }





    </style>
</head>
<body>
    <header>
        <h1>Project 3</h1>
    </header>

    <main>
        <div class="project">
           
           
            <iframe class="project-video" src="https://www.youtube.com/embed/hBNCrke3qqk?si=vX8i0CQJeimVxTUK" title="YouTube video" allowfullscreen width="560" height="315" style="margin-top: 20px; border: none;"></iframe>
            


            <div class="description">
                <p><h2><strong>MINImal Robot Dog</strong></h2></p>

                
                <div class="project-images">
                    <img src="media/minimal_simulation.png" alt="Project 1 Image">
                    <img src="media/minimal.jpg" alt="Project 1 Image">
                </div>
                <br>

                <p>
                While exploring several open source quadruped robot projects, I came across
SpotMiniMini. A linear policy trained offline using RL is used to control stability of the
robot. I wanted to explore the effect of using more sensors to provide the linear policy
with a more complete picture of the robot state, to see if it could improve the robot
performance. In the same spirit as the open-source projects that inspired mine, I
designed the robot to have a low hardware cost while expanding its sensing capabilities
to sense the terrain.
                </p><br>
            
                <p>
                    <b><h2>Training</h2></b>
                    <h4>Three sets of policies were trained:</h4>
<li>IMU policies, which used only inertial data as input</li>
<li>IMU+Contact policies, which additionally used foot-ground contact</li>
<li>IMU+Force policies, which additionally used ground reaction forces for each foot</li></p>

  <div class="project-images">
                        <img src="media/Minimal HD.gif" alt="Project 1 Image">
                        
                    </div>


<p>Enabling force sensing in simulation was kind of tricky, so I used Pybullet joint state
sensor to essentially emulate a force sensor. The lower leg has a small foot attached to
its end with a fixed joint. I wrote a function which fetched the joint state for this joint at
each simulation step, which included the “joint reaction forces”. The foot is small enough
that moments due to the ground reaction force can be ignored, so the joint reaction
forces can be directly used to closely approximate ground reactions. Using rotation
transforms, these forces are then converted from the joint frame to the robot frame.
From there, it was a simple matter of using a threshold function to emulate a contact
sensor: any force greater than the threshold value was treated as “foot in contact with
ground”.</p>

<p>I saw a huge increase in performance going from IMU to either IMU+Contacts or
IMU+Force. The force and contact-based policies have a few trade-offs between
themselves. Anticipating that contact detection would be relatively easier to implement
than 3-axis force sensing, I built a simple contact sensor using FSRs. While FSRs aren’t
really reliable for accurately measuring force, they worked sufficiently to detect contact.
Here’s a clip of my pilot test with a prototype contact sensor.</p>

                  
                   


                    <b><h2>Contact Sensing</h2></b>
                    <div class="project-images">
                        <img src="media/minimal_sensor.png" alt="Project 1 Image">

                        <img src="media/minimal_realsen.png" alt="Project 1 Image">
                    </div>


                    <p>>We explored various methods by which such contact estimation could be enabled. We chose FSR with a small form factor (10mm X 20 mm) and a force sensing range of 0.2N to 20N. The FSR is sandwiched between the lower-leg and foot of the robot. The lower leg and foot are separated by 4 silicon O-rings when assembled. When the foot contacts the ground, the O-rings are compressed and the cylindrical boss of the foot applies force on the FSR, decreasing its resistance. When the foot breaks contact with the ground, the O-rings decompress, causing the foot to return to its original position. The force applied on the FSR is decreased, increasing its resistance.</p>
                    
                
                    
                    <p>We use a voltage divider circuit to convert the varying 
                        resistance value to a voltage value, which is then read by a 
                        microcontroller. </p>
                
                    <div class="project-images">
                        <img src="media/contact_1.jpg" alt="Project 1 Image">
                        <img src="media/contact_2.jpg" alt="Project 1 Image">
                    </div>
                    <br>


                    <p><h4>Research </h4> <div class="research-description" style="text-align: left;">
                        <h4><a href="https://arxiv.org/pdf/2503.01102" target="_blank">Ground contact and reaction force sensing for linear policy control of quadruped robot</a></h4>
                      </div></p>



        
    </div>
    </main>


</body>
</html>
